import requests
import time
import numpy as np
from typing import List, Dict, Optional
from config import Config
from embedding_client import EmbeddingClient


class PaperRetriever:
    """è®ºæ–‡æ£€ç´¢å™¨ - åŸºäºSemantic Scholar APIï¼Œå¤±è´¥æ—¶fallbackåˆ°OpenAlex"""

    def __init__(self):
        self.config = Config
        self.embedding_client = None
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è®ºæ–‡æ£€ç´¢å™¨...")
        self._init_embedding_client()
        # OpenAlex API headersï¼ˆå»ºè®®åŒ…å«é‚®ç®±ï¼Œä½†éå¿…éœ€ï¼‰
        self.openalex_headers = {
            'User-Agent': 'ICAIS2025-PaperReview/1.0 ( https://github.com/your-repo )'
        }
        print("âœ… è®ºæ–‡æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")

    def _init_embedding_client(self):
        """åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯: {self.config.EMBEDDING_MODEL_NAME}...")
            self.embedding_client = EmbeddingClient()
            print(f"âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è¯­ä¹‰é‡æ’åº")
            self.embedding_client = None

    def _convert_openalex_to_semanticscholar_format(self, openalex_work: Dict) -> Dict:
        """å°†OpenAlexçš„workæ ¼å¼è½¬æ¢ä¸ºSemantic Scholaræ ¼å¼"""
        # æå–æ ‡é¢˜
        title = openalex_work.get('title', '') or ''
        
        # æå–æ‘˜è¦
        abstract = ''
        # OpenAlexçš„æ‘˜è¦å¯èƒ½åœ¨abstractå­—æ®µä¸­ï¼ˆå­—ç¬¦ä¸²ï¼‰æˆ–abstract_inverted_indexä¸­
        if 'abstract_inverted_index' in openalex_work and openalex_work['abstract_inverted_index']:
            try:
                inverted_index = openalex_work['abstract_inverted_index']
                # åˆ›å»ºä½ç½®åˆ°å•è¯çš„æ˜ å°„
                pos_to_word = {}
                for word, positions in inverted_index.items():
                    for pos in positions:
                        pos_to_word[pos] = word
                # æŒ‰ä½ç½®æ’åºå¹¶æ‹¼æ¥
                if pos_to_word:
                    sorted_positions = sorted(pos_to_word.keys())
                    abstract = ' '.join([pos_to_word[pos] for pos in sorted_positions])
            except Exception as e:
                print(f"âš ï¸  è½¬æ¢ OpenAlex æ‘˜è¦å¤±è´¥: {e}")
                abstract = ''
        elif 'abstract' in openalex_work and isinstance(openalex_work['abstract'], str):
            abstract = openalex_work['abstract']
        # å¦‚æœæ²¡æœ‰abstractï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
        if not abstract:
            abstract = ''
        
        # æå–paperIdï¼ˆä½¿ç”¨OpenAlexçš„IDï¼Œå»æ‰URLå‰ç¼€ï¼‰
        paper_id = openalex_work.get('id', '')
        if paper_id and isinstance(paper_id, str) and paper_id.startswith('https://openalex.org/'):
            paper_id = paper_id.replace('https://openalex.org/', '')
        elif not paper_id:
            # å¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºIDï¼ˆç”¨äºå»é‡ï¼‰
            paper_id = title
        
        return {
            'paperId': paper_id,
            'title': title,
            'abstract': abstract
        }

    def _get_papers_from_openalex(self, query: str, sort: str, max_results: int, timeout: int = 30) -> List[Dict]:
        """ä»OpenAlexè·å–è®ºæ–‡ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        url = "https://api.openalex.org/works"
        
        # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼šç§»é™¤å¼•å·å’Œç«–çº¿ï¼Œä¿ç•™è¿å­—ç¬¦å’Œå…¶ä»–å­—ç¬¦
        # å°† "keyword1" | "keyword2" | "keyword3" è½¬æ¢ä¸º keyword1 keyword2 keyword3
        cleaned_query = query.replace('"', '').replace(' | ', ' ').strip()
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
        import re
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
        
        params = {
            "search": cleaned_query,
            "sort": sort,
            "per_page": min(max_results, 200)  # OpenAlexæœ€å¤šè¿”å›200æ¡
        }
        
        try:
            response = requests.get(
                url, 
                params=params, 
                headers=self.openalex_headers,
                timeout=timeout
            )
            response.raise_for_status()
            data = response.json()
            
            if 'results' in data and data['results']:
                papers = []
                for work in data['results'][:max_results]:
                    paper = self._convert_openalex_to_semanticscholar_format(work)
                    # åªæ·»åŠ æœ‰æ ‡é¢˜çš„è®ºæ–‡
                    if paper.get('title', '').strip():
                        papers.append(paper)
                return papers
            return []
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 400:
                print(f"âŒ OpenAlex æ£€ç´¢å¤±è´¥ (400 Bad Request): {e}")
                print(f"   è¯·æ±‚ URL: {e.request.url}")
                try:
                    error_text = e.response.text[:200]
                    print(f"   å“åº”å†…å®¹: {error_text}...")
                except:
                    pass
            else:
                print(f"âŒ OpenAlex æ£€ç´¢å¤±è´¥ (HTTP {e.response.status_code}): {e}")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸  OpenAlexæ£€ç´¢å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âš ï¸  OpenAlexæ£€ç´¢å¼‚å¸¸: {e}")
            return []

    def get_newest_paper_openalex(self, query: str, max_results: Optional[int] = None) -> List[Dict]:
        """ä½¿ç”¨OpenAlexè·å–æœ€æ–°è®ºæ–‡"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        print(f"ğŸ”„ å°è¯•ä½¿ç”¨OpenAlexè·å–æœ€æ–°è®ºæ–‡...")
        return self._get_papers_from_openalex(query, "publication_date:desc", max_results)

    def get_highly_cited_paper_openalex(self, query: str, max_results: Optional[int] = None) -> List[Dict]:
        """ä½¿ç”¨OpenAlexè·å–é«˜å¼•ç”¨è®ºæ–‡"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        print(f"ğŸ”„ å°è¯•ä½¿ç”¨OpenAlexè·å–é«˜å¼•ç”¨è®ºæ–‡...")
        return self._get_papers_from_openalex(query, "cited_by_count:desc", max_results)

    def get_relevant_paper_openalex(self, query: str, max_results: Optional[int] = None) -> List[Dict]:
        """ä½¿ç”¨OpenAlexè·å–ç›¸å…³è®ºæ–‡ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        print(f"ğŸ”„ å°è¯•ä½¿ç”¨OpenAlexè·å–ç›¸å…³è®ºæ–‡...")
        # OpenAlexä¸æ”¯æŒ"relevance"æ’åºï¼Œä½¿ç”¨cited_by_countä½œä¸ºæ›¿ä»£ï¼ˆé«˜å¼•ç”¨é€šå¸¸æ›´ç›¸å…³ï¼‰
        return self._get_papers_from_openalex(query, "cited_by_count:desc", max_results)

    def get_newest_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–æœ€æ–°è®ºæ–‡ï¼ˆSemantic Scholarå¤±è´¥æ—¶fallbackåˆ°OpenAlexï¼‰"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        # å‡å°‘Semantic Scholarçš„é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿfallbackåˆ°OpenAlex
        max_retries = min(max_retries or 2, 2)  # æœ€å¤šé‡è¯•2æ¬¡

        url = "http://api.semanticscholar.org/graph/v1/paper/search/bulk"
        params = {"query": query, "fields": "title,abstract,paperId", "sort": "publicationDate:desc"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œç‰¹åˆ«æ˜¯429é”™è¯¯
                if response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_newest_paper_openalex(query, max_results)
                
                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´
                        print(f"è·å–æœ€æ–°è®ºæ–‡å¤±è´¥ (HTTP {response.status_code})ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âš ï¸  Semantic Scholarè·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                        return self.get_newest_paper_openalex(query, max_results)
                
                data = response.json()

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–æœ€æ–°è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–æœ€æ–°è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–æœ€æ–°è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–æœ€æ–°è®ºæ–‡è¶…æ—¶ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_newest_paper_openalex(query, max_results)
            except requests.exceptions.RequestException as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_newest_paper_openalex(query, max_results)
                
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–æœ€æ–°è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_newest_paper_openalex(query, max_results)
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–æœ€æ–°è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_newest_paper_openalex(query, max_results)

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œfallbackåˆ°OpenAlex
        print(f"âš ï¸  Semantic Scholarè·å–æœ€æ–°è®ºæ–‡å¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
        return self.get_newest_paper_openalex(query, max_results)

    def get_highly_cited_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–é«˜å¼•ç”¨è®ºæ–‡ï¼ˆSemantic Scholarå¤±è´¥æ—¶fallbackåˆ°OpenAlexï¼‰"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        # å‡å°‘Semantic Scholarçš„é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿfallbackåˆ°OpenAlex
        max_retries = min(max_retries or 2, 2)  # æœ€å¤šé‡è¯•2æ¬¡

        url = "http://api.semanticscholar.org/graph/v1/paper/search/bulk"
        params = {"query": query, "fields": "title,abstract,paperId", "sort": "citationCount:desc"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œç‰¹åˆ«æ˜¯429é”™è¯¯
                if response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_highly_cited_paper_openalex(query, max_results)
                
                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´
                        print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥ (HTTP {response.status_code})ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âš ï¸  Semantic Scholarè·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                        return self.get_highly_cited_paper_openalex(query, max_results)
                
                data = response.json()

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–é«˜å¼•ç”¨è®ºæ–‡è¶…æ—¶ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_highly_cited_paper_openalex(query, max_results)
            except requests.exceptions.RequestException as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_highly_cited_paper_openalex(query, max_results)
                
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_highly_cited_paper_openalex(query, max_results)
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_highly_cited_paper_openalex(query, max_results)

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œfallbackåˆ°OpenAlex
        print(f"âš ï¸  Semantic Scholarè·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
        return self.get_highly_cited_paper_openalex(query, max_results)

    def get_relevant_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–ç›¸å…³è®ºæ–‡ï¼ˆSemantic Scholarå¤±è´¥æ—¶fallbackåˆ°OpenAlexï¼‰"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        # å‡å°‘Semantic Scholarçš„é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿfallbackåˆ°OpenAlex
        max_retries = min(max_retries or 2, 2)  # æœ€å¤šé‡è¯•2æ¬¡

        url = "http://api.semanticscholar.org/graph/v1/paper/search"
        params = {"query": query, "fields": "title,abstract,paperId"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œç‰¹åˆ«æ˜¯429é”™è¯¯
                if response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_relevant_paper_openalex(query, max_results)
                
                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´
                        print(f"è·å–ç›¸å…³è®ºæ–‡å¤±è´¥ (HTTP {response.status_code})ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                        return self.get_relevant_paper_openalex(query, max_results)
                
                try:
                    data = response.json()
                except ValueError:
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–ç›¸å…³è®ºæ–‡JSONè§£æå¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡JSONè§£æå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                        return self.get_relevant_paper_openalex(query, max_results)

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–ç›¸å…³è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 2)
                        print(f"è·å–ç›¸å…³è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–ç›¸å…³è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡è¶…æ—¶ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_relevant_paper_openalex(query, max_results)
            except requests.exceptions.RequestException as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                    print(f"âš ï¸  Semantic Scholarè¿”å›429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤šï¼‰ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_relevant_paper_openalex(query, max_results)
                
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–ç›¸å…³è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_relevant_paper_openalex(query, max_results)
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 2)
                    print(f"è·å–ç›¸å…³è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
                    return self.get_relevant_paper_openalex(query, max_results)

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œfallbackåˆ°OpenAlex
        print(f"âš ï¸  Semantic Scholarè·å–ç›¸å…³è®ºæ–‡å¤±è´¥ï¼Œåˆ‡æ¢åˆ°OpenAlex...")
        return self.get_relevant_paper_openalex(query, max_results)

    def merge_and_deduplicate(self, results: Dict[str, List[Dict]]) -> List[Dict]:
        """èåˆå’Œå»é‡è®ºæ–‡"""
        seen_ids = set()
        all_papers = []

        for paper_list in results.values():
            for paper in paper_list:
                paper_id = paper.get('paperId') or paper.get('title', '')
                if paper_id and paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    all_papers.append(paper)

        return all_papers

    def rerank_by_similarity(self, papers: List[Dict], background_embedding: np.ndarray, background_text: str) -> List[Dict]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦é‡æ’åºè®ºæ–‡"""
        if not self.embedding_client or len(papers) == 0:
            return papers

        try:
            paper_texts = []
            for paper in papers:
                abstract = paper.get('abstract', '') or ''
                title = paper.get('title', '') or ''
                text = f"{title} {abstract}".strip()
                paper_texts.append(text if text else " ")

            paper_embeddings = self.embedding_client.encode(paper_texts, show_progress_bar=False)
            
            if paper_embeddings.ndim == 1:
                paper_embeddings = paper_embeddings.reshape(1, -1)

            similarities = []
            for paper_emb in paper_embeddings:
                similarity = np.dot(background_embedding, paper_emb) / (
                    np.linalg.norm(background_embedding) * np.linalg.norm(paper_emb) + 1e-8
                )
                similarities.append(similarity)

            sorted_papers = sorted(
                zip(papers, similarities),
                key=lambda x: x[1],
                reverse=True
            )

            return [paper for paper, _ in sorted_papers]

        except Exception as e:
            print(f"âš ï¸  è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return papers

    def hybrid_retrieve(self, query_text: str, keywords: List[str]) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ - ä¼˜å…ˆä½¿ç”¨Semantic Scholar APIï¼Œå¤±è´¥æ—¶è‡ªåŠ¨fallbackåˆ°OpenAlex
        """
        if len(keywords) == 1:
            query = keywords[0]
        else:
            query = " | ".join(f'"{item}"' for item in keywords)

        import concurrent.futures

        newest_papers = []
        highly_cited_papers = []
        relevant_papers = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_newest = executor.submit(self.get_newest_paper, query)
            future_highly_cited = executor.submit(self.get_highly_cited_paper, query)
            future_relevant = executor.submit(self.get_relevant_paper, query)

            try:
                newest_papers = future_newest.result(timeout=120)
            except Exception:
                newest_papers = []

            try:
                highly_cited_papers = future_highly_cited.result(timeout=120)
            except Exception:
                highly_cited_papers = []

            try:
                relevant_papers = future_relevant.result(timeout=120)
            except Exception:
                relevant_papers = []

        results = {
            "newest_papers": newest_papers or [],
            "highly_cited_papers": highly_cited_papers or [],
            "relevant_papers": relevant_papers or []
        }
        all_papers = self.merge_and_deduplicate(results)

        if not all_papers:
            return []

        if self.embedding_client:
            try:
                background_embedding = self.embedding_client.encode(query_text, show_progress_bar=False)
                if background_embedding is not None and len(background_embedding) > 0:
                    all_papers = self.rerank_by_similarity(all_papers, background_embedding, query_text)
            except Exception as e:
                print(f"âš ï¸  è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")

        return all_papers[:self.config.MAX_TOTAL_PAPERS]

